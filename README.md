# Churn Classification using PySpark

## Intro
This project aims to create a machine learning model using PySpark. As we all know currently big data is used by a lot of big companies, the common library can no longer handle big data which is why we need big data tools. One of the most popular big data tools is Apache Spark. Spark was written in Scala but with PySpark we can utilize Spark using Python language. 

The dataset is from [Kaggle](https://www.kaggle.com/datasets/parisanahmadi/bank-data-churn-classification).  

## Tools
- PySpark
- Jupyter Notebook
- Oracle VM VirtualBox Manager to host Linux

## Work Stages
- Environment preparation (import libraries and start a spark session)
- Data preprocession (handling missing value, string indexer, one-hot encoder)
- Assembling data
- Train test split
- Standardizing
- Model fitting and evaluation
- Feature importance
